{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## crawl taiwan.cn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-06-28T14:31:42.951861Z",
     "start_time": "2020-06-28T14:31:42.948961Z"
    }
   },
   "outputs": [],
   "source": [
    "import urllib.request\n",
    "from bs4 import BeautifulSoup as Soup\n",
    "from tqdm import tqdm\n",
    "\n",
    "# url = \"https://jwlin.github.io/py-scraping-analysis-book/ch1/connect.html\"\n",
    "title = []\n",
    "abstract = []\n",
    "content = []\n",
    "time = []\n",
    "\n",
    "for page_num in tqdm(range(1, 47)):\n",
    "    url = 'http://www.taiwan.cn/taiwan/index_'+str(page_num)+'.htm'\n",
    "    res_p = requests.get(url)\n",
    "    res_p.encoding='gbk'\n",
    "    page_html = Soup(res_p.text, \"html.parser\")\n",
    "    \n",
    "    top_r = page_html.find_all('div', class_='top')[0]\n",
    "    title.append(top_r.find('h2').text)\n",
    "    abstract.append(top_r.find_all('p')[0].text)\n",
    "    full_content = requests.get(top_r.find_all('p')[0].find('a')['href'])\n",
    "    full_content.encoding='gbk'\n",
    "    full_content_html = Soup(full_content.text, \"html.parser\")\n",
    "    content.append(full_content_html.find_all('div', class_='TRS_Editor')[0].text)\n",
    "    time.append(top_r.find('span').text)\n",
    "\n",
    "    rest_r = page_html.find_all('ul', class_='list')\n",
    "    rest_r = rest_r[0].find_all('li')\n",
    "    for r in rest_r:\n",
    "#         print('--------------------------------')\n",
    "        title.append(r.find_all('h2')[0].text)\n",
    "        abstract.append(r.find_all('p')[0].text)\n",
    "    #     print(r.find_all('p')[0].find('a')['href'])\n",
    "        time.append(r.find_all('p', class_='info')[0].find('span').text)\n",
    "        full_content = requests.get(r.find_all('p')[0].find('a')['href'])\n",
    "        full_content.encoding='gbk'\n",
    "        full_content_html = Soup(full_content.text, \"html.parser\")\n",
    "        content.append(full_content_html.find_all('div', class_='TRS_Editor')[0].text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "url = 'http://www.taiwan.cn/taiwan/index.htm'\n",
    "res_p = requests.get(url)\n",
    "res_p.encoding='gbk'\n",
    "page_html = Soup(res_p.text, \"html.parser\")\n",
    "\n",
    "top_r = page_html.find_all('div', class_='top')[0]\n",
    "title.append(top_r.find('h2').text)\n",
    "abstract.append(top_r.find_all('p')[0].text)\n",
    "full_content = requests.get(top_r.find_all('p')[0].find('a')['href'])\n",
    "full_content.encoding='gbk'\n",
    "full_content_html = Soup(full_content.text, \"html.parser\")\n",
    "content.append(full_content_html.find_all('div', class_='TRS_Editor')[0].text)\n",
    "time.append(top_r.find('span').text)\n",
    "\n",
    "rest_r = page_html.find_all('ul', class_='list')\n",
    "rest_r = rest_r[0].find_all('li')\n",
    "for r in rest_r:\n",
    "#         print('--------------------------------')\n",
    "    title.append(r.find_all('h2')[0].text)\n",
    "    abstract.append(r.find_all('p')[0].text)\n",
    "#     print(r.find_all('p')[0].find('a')['href'])\n",
    "    time.append(r.find_all('p', class_='info')[0].find('span').text)\n",
    "    full_content = requests.get(r.find_all('p')[0].find('a')['href'])\n",
    "    full_content.encoding='gbk'\n",
    "    full_content_html = Soup(full_content.text, \"html.parser\")\n",
    "    content.append(full_content_html.find_all('div', class_='TRS_Editor')[0].text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(len(title),  len(time), len(abstract), len(content))\n",
    "import pandas as pd  \n",
    "dict_ = {'title':title,  'time':time, 'abstract':abstract, 'content':content}\n",
    "df = pd.DataFrame(dict_)\n",
    "df.to_csv('./twcn.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## crawl storm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "title = []\n",
    "tag = []\n",
    "author = []\n",
    "time = []\n",
    "abstract = []\n",
    "a_content = []\n",
    "for page in tqdm(range(450)):\n",
    "    keyword = quote('\"蔡英文\"'.encode('utf8'))\n",
    "#     print(page)\n",
    "    url = 'https://www.storm.mg/site-search/result/' + str(page) +'?q={}&format=nolimit'.format(keyword)\n",
    "    req = urllib.request.Request(url, headers={'User-Agent': 'Mozilla/5.0 (X11; Ubuntu; Linux i686; rv:64.0) Gecko/20100101 Firefox/64.0'})\n",
    "    response = urllib.request.urlopen(req)\n",
    "    page = response.read()\n",
    "    content = Soup(page, \"html.parser\")\n",
    "    reps = content.find_all('div', class_='card_inner_wrapper')\n",
    "    for rep in reps[20:]:\n",
    "#         print(rep)\n",
    "        title.append(rep.find(class_='card_title').text)\n",
    "        tag.append([tag.text for tag in rep.find_all(class_='card_tag')])\n",
    "        author.append(rep.find(class_='info_author').text)\n",
    "        time.append(rep.find(class_='info_time').text)\n",
    "        abstract.append(rep.find(class_='card_substance').text)\n",
    "        url_cont = 'https://www.storm.mg/' + rep.find(class_='card_substance')['href']\n",
    "        req_cont = urllib.request.Request(url_cont, headers={'User-Agent': 'Mozilla/5.0 (X11; Ubuntu; Linux i686; rv:64.0) Gecko/20100101 Firefox/64.0'})\n",
    "        response_cont = urllib.request.urlopen(req_cont)\n",
    "        page_cont = response_cont.read()\n",
    "        content_cont = Soup(page_cont, \"html.parser\")\n",
    "        a_content.append(content_cont.find('article').text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
